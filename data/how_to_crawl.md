# å°ç£å°ˆåˆ©å•†æ¨™é–‹æ”¾è³‡æ–™ä¸‹è¼‰æ•™å­¸

## ğŸ“‹ ç›®éŒ„
- [é—œæ–¼é–‹æ”¾è³‡æ–™](#é—œæ–¼é–‹æ”¾è³‡æ–™)
- [è³‡æ–™ä¾†æº](#è³‡æ–™ä¾†æº)
- [ç¶²é çµæ§‹èªªæ˜](#ç¶²é çµæ§‹èªªæ˜)
- [ä¸‹è¼‰æ–¹å¼](#ä¸‹è¼‰æ–¹å¼)
  - [æ–¹æ³•ä¸€ï¼šä½¿ç”¨ FTP è»Ÿé«”ä¸‹è¼‰](#æ–¹æ³•ä¸€ä½¿ç”¨-ftp-è»Ÿé«”ä¸‹è¼‰)
  - [æ–¹æ³•äºŒï¼šçˆ¬å–ä¸‹è¼‰é€£çµ](#æ–¹æ³•äºŒçˆ¬å–ä¸‹è¼‰é€£çµ)
  - [æ–¹æ³•ä¸‰ï¼šä½¿ç”¨ç¨‹å¼åŒ–æ‰¹æ¬¡ä¸‹è¼‰](#æ–¹æ³•ä¸‰ä½¿ç”¨ç¨‹å¼åŒ–æ‰¹æ¬¡ä¸‹è¼‰)
- [è³‡æ–™é›†èªªæ˜](#è³‡æ–™é›†èªªæ˜)
- [å¸¸è¦‹å•é¡Œ](#å¸¸è¦‹å•é¡Œ)

---

## é—œæ–¼é–‹æ”¾è³‡æ–™

ç¶“æ¿Ÿéƒ¨æ™ºæ…§è²¡ç”¢å±€æä¾›å°ˆåˆ©èˆ‡å•†æ¨™çš„é–‹æ”¾è³‡æ–™ä¾›æ°‘çœ¾å…è²»ä¸‹è¼‰ä½¿ç”¨ï¼ŒåŒ…å«ï¼š
- ç™¼æ˜å°ˆåˆ©å…¬é–‹å…¬å ±
- ç™¼æ˜å°ˆåˆ©å…¬å‘Šå…¬å ±
- å°ˆåˆ©èªªæ˜æ›¸
- å•†æ¨™è¨»å†Šæ¡ˆå…¬å ±
- å…¶ä»–ç›¸é—œè³‡æ–™

**å®˜æ–¹ç¶²ç«™**ï¼šhttps://cloud.tipo.gov.tw/S220/opdata/search/trademark

âš ï¸ **é‡è¦æé†’**ï¼š
- æœ¬ç³»çµ±æä¾›çš„è³‡æ–™é›†åƒ…ä¾›åŠ å€¼é‹ç”¨ï¼Œä¸ä½œç‚ºå‡†é§ä¾æ“š
- æ‰€æœ‰æ­£å¼è³‡æ–™è«‹ä»¥ç¶“æ¿Ÿéƒ¨æ™ºæ…§è²¡ç”¢å±€å…¬å‘Šç‚ºæº–

---

## è³‡æ–™ä¾†æº

### ä¸»è¦è³‡æ–™é›†ç¶²ç«™
- **å°ˆåˆ©å•†æ¨™é–‹æ”¾è³‡æ–™ä¸‹è¼‰**ï¼šhttps://cloud.tipo.gov.tw/S220/opdata
- **å•†æ¨™è³‡æ–™é›†è©³ç´°é **ï¼šhttps://cloud.tipo.gov.tw/S220/opdata/detail/TrademarkRegXMLA

### å…¶ä»–åƒè€ƒè³‡æº
- æ™ºæ…§è²¡ç”¢å±€å®˜ç¶²ï¼šhttps://www.tipo.gov.tw
- å…¨çƒå°ˆåˆ©æª¢ç´¢ç³»çµ± (GPSS)ï¼šhttps://gpss.tipo.gov.tw
- æ”¿åºœè³‡æ–™é–‹æ”¾å¹³å°ï¼šhttps://data.gov.tw

---

## ç¶²é çµæ§‹èªªæ˜

### é–‹æ”¾è³‡æ–™ç¶²ç«™ä»‹é¢

æ™ºæ…§è²¡ç”¢å±€çš„é–‹æ”¾è³‡æ–™ç¶²ç«™ (https://cloud.tipo.gov.tw/S220/opdata/detail/TrademarkRegXMLA) æ¡ç”¨ JavaScript å‹•æ…‹æ¸²æŸ“ï¼Œçµæ§‹å¦‚ä¸‹ï¼š

#### 1. å¹´ä»½é¸æ“‡å™¨
- ä½ç½®ï¼šé é¢ä¸Šæ–¹
- é¡¯ç¤ºæ ¼å¼ï¼šã€Œæ­·å²å·æœŸ 114 å¹´ã€
- åŠŸèƒ½ï¼šä¸‹æ‹‰é¸å–®å¯é¸æ“‡ä¸åŒæ°‘åœ‹å¹´ä»½ï¼ˆé€šå¸¸å¾ 50 å¹´è‡³ä»Šï¼‰

#### 2. æœŸåˆ¥åˆ—è¡¨
- **å¸ƒå±€**ï¼šå…©æ¬„å¼å¡ç‰‡/è¡¨æ ¼æ’åˆ—
- **æ¯å€‹æœŸåˆ¥é …ç›®åŒ…å«**ï¼š
  - å·æœŸæ¨™é¡Œï¼šã€Œç¬¬ XX å· YY æœŸã€
  - ç™¼å¸ƒæ—¥æœŸï¼šæ°‘åœ‹å¹´-æœˆ-æ—¥æ ¼å¼ï¼ˆä¾‹ï¼š114-10-18ï¼‰
  - æª”æ¡ˆå¤§å°ï¼šæ‹¬è™Ÿé¡¯ç¤ºï¼ˆä¾‹ï¼š77.22 MBï¼‰
  - ä¸‹è¼‰æŒ‰éˆ•ï¼šè—è‰²ã€Œä¸‹è¼‰è·¯å¾‘ã€é€£çµ

#### 3. è³‡æ–™ç‰¹æ€§
- **å·è™Ÿè¦å¾‹**ï¼šåŒä¸€å¹´ä»½å…§å·è™Ÿé€šå¸¸å›ºå®š
- **æœŸè™Ÿæ’åˆ—**ï¼šç”±æ–°åˆ°èˆŠæ’åˆ—
- **ç™¼å¸ƒé »ç‡**ï¼šç´„æ¯ 2 é€±ç™¼å¸ƒä¸€æœŸ
- **æª”æ¡ˆå¤§å°**ï¼šå•†æ¨™è³‡æ–™æ¯æœŸç´„ 80-130 MB

### å¤šå¹´ä»½å¤šæœŸè³‡æ–™çµæ§‹

ç”±æ–¼ç¶²ç«™åŒ…å«ï¼š
- âœ… **50+ å€‹å¹´ä»½**ï¼ˆæ°‘åœ‹ 50 å¹´è‡³ä»Šï¼‰
- âœ… **æ¯å¹´ 20+ æœŸ**ï¼ˆä¾å¯¦éš›å…¬å ±ç™¼å¸ƒæ•¸é‡ï¼‰
- âœ… **æ•¸åƒå€‹ä¸‹è¼‰é€£çµ**

å› æ­¤å»ºè­°ä½¿ç”¨**è‡ªå‹•åŒ–å·¥å…·**æ‰¹æ¬¡å–å¾—ä¸‹è¼‰é€£çµå’Œæª”æ¡ˆã€‚

---

## ä¸‹è¼‰æ–¹å¼

### ç‚ºä»€éº¼éœ€è¦ä½¿ç”¨ FTP è»Ÿé«”ï¼Ÿ

ç”±æ–¼æ™ºæ…§è²¡ç”¢å±€æä¾›çš„é–‹æ”¾è³‡æ–™æ¡ç”¨ **FTP å‚³è¼¸å”å®š**ï¼Œè€Œç›®å‰å¤šæ•¸ç€è¦½å™¨ï¼ˆChromeã€Edgeã€Firefox ç­‰ï¼‰å·²ä¸æ”¯æ´ FTPï¼Œä½¿ç”¨ç€è¦½å™¨ç€è¦½æ™‚å¯èƒ½æœƒé¡¯ç¤ºç©ºç™½é é¢ã€‚

**å»ºè­°ä½¿ç”¨**ï¼š
- FTP è»Ÿé«”ï¼šFileZillaã€WinSCP
- ç¨‹å¼åŒ–å·¥å…·ï¼šPythonã€curl ç­‰

---

## æ–¹æ³•ä¸€ï¼šä½¿ç”¨ FTP è»Ÿé«”ä¸‹è¼‰

### æ­¥é©Ÿ 1ï¼šä¸‹è¼‰ä¸¦å®‰è£ FileZilla

1. å‰å¾€ [FileZilla å®˜æ–¹ç¶²ç«™](https://filezilla-project.org/)
2. ä¸‹è¼‰ **FileZilla Client**ï¼ˆä¾æ‚¨çš„ä½œæ¥­ç³»çµ±é¸æ“‡ï¼‰
3. å®Œæˆè»Ÿé«”å®‰è£ä¸¦åŸ·è¡Œç¨‹å¼

### æ­¥é©Ÿ 2ï¼šè¤‡è£½ FTP é€£ç·šè³‡è¨Š

1. å‰å¾€ [é–‹æ”¾è³‡æ–™ç¶²ç«™](https://cloud.tipo.gov.tw/S220/opdata/detail/TrademarkRegXMLA)
2. é¸æ“‡æ‚¨è¦ä¸‹è¼‰çš„è³‡æ–™é›†ï¼ˆä¾‹å¦‚ï¼šå•†æ¨™è¨»å†Šæ¡ˆå…¬å ±ï¼‰
3. æ‰¾åˆ°ç‰¹å®šå·æœŸçš„ä¸‹è¼‰è·¯å¾‘
4. åœ¨ä¸‹è¼‰è·¯å¾‘ä¸Šé»é¸æ»‘é¼ å³éµ
5. é¸æ“‡ã€Œè¤‡è£½é€£çµç¶²å€ã€

**ç¯„ä¾‹ FTP ç¶²å€æ ¼å¼**ï¼š
```
ftp://opdata.tipo.gov.tw/TrademarkReg/50/5/
```

### æ­¥é©Ÿ 3ï¼šä½¿ç”¨ FileZilla é€£ç·š

1. é–‹å•Ÿ FileZilla è»Ÿé«”
2. åœ¨ä¸Šæ–¹ã€Œä¸»æ©Ÿã€æ¬„ä½è²¼ä¸Šå‰›æ‰è¤‡è£½çš„ FTP ç¶²å€
3. é»é¸ã€Œå¿«é€Ÿé€£ç·šã€æŒ‰éˆ•
4. å¦‚æœå‡ºç¾ç¢ºèªè¦–çª—ï¼Œé»é¸ã€Œç¢ºå®šã€

![FileZilla é€£ç·šç¤ºæ„](ç¤ºæ„åœ–)

### æ­¥é©Ÿ 4ï¼šä¸‹è¼‰æª”æ¡ˆ

1. é€£ç·šæˆåŠŸå¾Œï¼Œå³å´è¦–çª—æœƒé¡¯ç¤º FTP ä¼ºæœå™¨ä¸Šçš„æª”æ¡ˆ
2. é¸æ“‡æ‚¨éœ€è¦çš„æª”æ¡ˆæˆ–è³‡æ–™å¤¾
3. é»é¸æ»‘é¼ å³éµé¸æ“‡ã€Œä¸‹è¼‰ã€
4. æˆ–ç›´æ¥æ‹–æ›³æª”æ¡ˆåˆ°å·¦å´çš„æœ¬æ©Ÿè³‡æ–™å¤¾

**ä¸‹è¼‰ä½ç½®**ï¼š
- å·¦å´è¦–çª—ï¼šæ‚¨çš„é›»è…¦æœ¬æ©Ÿæª”æ¡ˆ
- å³å´è¦–çª—ï¼šFTP ä¼ºæœå™¨æª”æ¡ˆ

### æ­¥é©Ÿ 5ï¼šè§£æ±ºé€£ç·šé€¾æ™‚å•é¡Œï¼ˆé¸ç”¨ï¼‰

å¦‚æœé‡åˆ°é€£ç·šé€¾æ™‚çš„æƒ…æ³ï¼š

1. é»é¸ä¸Šæ–¹é¸å–®ã€Œç·¨è¼¯ã€â†’ã€Œè¨­å®šã€
2. é¸æ“‡ã€Œé€£ç·šã€
3. åœ¨å³å´è¼¸å…¥æ¡†å°‡é€¾æ™‚è¨­å®šæ”¹ç‚º **240 ç§’**æˆ– **0 ç§’**ï¼ˆ0 è¡¨ç¤ºä¸é™æ™‚ï¼‰
4. é»é¸ã€Œç¢ºå®šã€å„²å­˜è¨­å®š

---

## æ–¹æ³•äºŒï¼šçˆ¬å–ä¸‹è¼‰é€£çµ

ç”±æ–¼ç¶²ç«™æœ‰æ•¸åå€‹å¹´ä»½ã€æ¯å¹´æ•¸åæœŸï¼Œæ‰‹å‹•è¤‡è£½é€£çµéå¸¸è€—æ™‚ã€‚å»ºè­°ä½¿ç”¨è‡ªå‹•åŒ–å·¥å…·æ‰¹æ¬¡å–å¾—æ‰€æœ‰ FTP ä¸‹è¼‰é€£çµã€‚

### ä½¿ç”¨ Playwright çˆ¬å–ï¼ˆæ¨è–¦ï¼‰

Playwright å¯ä»¥è™•ç† JavaScript å‹•æ…‹æ¸²æŸ“çš„ç¶²é ï¼Œé©åˆçˆ¬å–æ­¤ç¶²ç«™ã€‚

#### å®‰è£ Playwright

```bash
# å®‰è£ Playwright
pip install playwright

# å®‰è£ç€è¦½å™¨
playwright install chromium
```

#### çˆ¬å–è…³æœ¬ç¯„ä¾‹

```python
from playwright.sync_api import sync_playwright
import json
import time

def scrape_tipo_data(url, output_file='tipo_download_links.json'):
    """
    çˆ¬å–æ™ºæ…§è²¡ç”¢å±€é–‹æ”¾è³‡æ–™ç¶²ç«™çš„æ‰€æœ‰ä¸‹è¼‰é€£çµ
    
    Args:
        url: è³‡æ–™é›†ç¶²å€
        output_file: è¼¸å‡ºçš„ JSON æª”æ¡ˆåç¨±
    """
    with sync_playwright() as p:
        # å•Ÿå‹•ç€è¦½å™¨
        browser = p.chromium.launch(headless=False)  # headless=True å¯éš±è—ç€è¦½å™¨è¦–çª—
        page = browser.new_page()
        
        print(f"æ­£åœ¨è¨ªå•ç¶²é : {url}")
        page.goto(url, wait_until='networkidle')
        
        # ç­‰å¾…é é¢è¼‰å…¥
        time.sleep(3)
        
        all_data = {}
        
        # æ‰¾åˆ°å¹´ä»½é¸æ“‡å™¨ï¼ˆéœ€æ ¹æ“šå¯¦éš› HTML çµæ§‹èª¿æ•´ï¼‰
        year_selector = 'select'  # æˆ–å…¶ä»–é¸æ“‡å™¨
        
        # å–å¾—æ‰€æœ‰å¯é¸å¹´ä»½
        years = page.locator(year_selector + ' option').all_inner_texts()
        print(f"æ‰¾åˆ° {len(years)} å€‹å¹´ä»½")
        
        for year_text in years:
            year = year_text.strip().replace('å¹´', '')
            print(f"\nè™•ç†å¹´ä»½: {year}")
            
            # é¸æ“‡å¹´ä»½
            page.select_option(year_selector, label=year_text)
            time.sleep(2)  # ç­‰å¾…è³‡æ–™è¼‰å…¥
            
            # æ‰¾åˆ°æ‰€æœ‰æœŸåˆ¥ï¼ˆæ ¹æ“šæˆªåœ–ï¼Œä¸‹è¼‰è·¯å¾‘æ˜¯è—è‰²é€£çµï¼‰
            download_links = page.locator('a:has-text("ä¸‹è¼‰è·¯å¾‘")').all()
            
            year_data = []
            
            for link in download_links:
                # å–å¾—è©²æœŸåˆ¥çš„è³‡è¨Š
                parent = link.locator('xpath=../..').first  # å¾€ä¸Šæ‰¾åˆ°åŒ…å«å®Œæ•´è³‡è¨Šçš„å®¹å™¨
                
                # æ“·å–å·æœŸã€æ—¥æœŸã€æª”æ¡ˆå¤§å°ï¼ˆéœ€æ ¹æ“šå¯¦éš›çµæ§‹èª¿æ•´ï¼‰
                text = parent.inner_text()
                
                # å–å¾— FTP é€£çµ
                ftp_url = link.get_attribute('href')
                
                period_info = {
                    'text': text.strip(),
                    'ftp_url': ftp_url
                }
                
                year_data.append(period_info)
                print(f"  - {text.split()[0] if text else 'Unknown'}: {ftp_url}")
            
            all_data[year] = year_data
        
        # å„²å­˜ç‚º JSON
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ“ å®Œæˆï¼å…±æ“·å– {sum(len(v) for v in all_data.values())} å€‹ä¸‹è¼‰é€£çµ")
        print(f"âœ“ å·²å„²å­˜è‡³: {output_file}")
        
        browser.close()

# ä½¿ç”¨ç¯„ä¾‹
if __name__ == '__main__':
    url = 'https://cloud.tipo.gov.tw/S220/opdata/detail/TrademarkRegXMLA'
    scrape_tipo_data(url)
```

#### æ›´ç²¾ç¢ºçš„çˆ¬å–è…³æœ¬ï¼ˆéœ€æª¢æŸ¥å¯¦éš› HTMLï¼‰

```python
from playwright.sync_api import sync_playwright
import json
import re

def scrape_tipo_detailed(url='https://cloud.tipo.gov.tw/S220/opdata/detail/TrademarkRegXMLA'):
    """ç²¾ç¢ºçˆ¬å–ï¼ŒåŒ…å«å·æœŸã€æ—¥æœŸã€æª”æ¡ˆå¤§å°ç­‰è³‡è¨Š"""
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        page.goto(url, wait_until='networkidle')
        page.wait_for_timeout(3000)
        
        all_data = {}
        
        # ä½¿ç”¨é–‹ç™¼è€…å·¥å…·æª¢æŸ¥å¯¦éš›çš„é¸æ“‡å™¨
        # é€™è£¡æä¾›å¸¸è¦‹çš„å¯èƒ½æ€§ï¼Œéœ€æ ¹æ“šå¯¦éš›æƒ…æ³èª¿æ•´
        
        year_options = page.locator('select option').all()
        
        for option in year_options:
            year = option.inner_text().strip()
            if not year or 'é¸æ“‡' in year:
                continue
                
            print(f"\nè™•ç†: {year}")
            option.click()
            page.wait_for_timeout(2000)
            
            # æ‰¾å‡ºæ‰€æœ‰æœŸåˆ¥é …ç›®
            # æ ¹æ“šæˆªåœ–ï¼Œæ¯å€‹æœŸåˆ¥å¯èƒ½åœ¨ <div> æˆ– <tr> ä¸­
            items = page.locator('.period-item, tr').all()  # éœ€èª¿æ•´
            
            year_data = []
            
            for item in items:
                try:
                    # å–å¾—æ–‡å­—è³‡è¨Š
                    text = item.inner_text()
                    
                    # è§£æå·æœŸè™Ÿï¼ˆä¾‹ï¼šç¬¬ 52 å· 20 æœŸï¼‰
                    volume_match = re.search(r'ç¬¬\s*(\d+)\s*å·\s*(\d+)\s*æœŸ', text)
                    
                    # è§£ææ—¥æœŸï¼ˆä¾‹ï¼š114-10-18ï¼‰
                    date_match = re.search(r'(\d{3}-\d{2}-\d{2})', text)
                    
                    # è§£ææª”æ¡ˆå¤§å°ï¼ˆä¾‹ï¼š(77.22 MB)ï¼‰
                    size_match = re.search(r'\(([0-9.]+\s*[MG]B)\)', text)
                    
                    # å–å¾— FTP é€£çµ
                    link = item.locator('a:has-text("ä¸‹è¼‰è·¯å¾‘")').first
                    ftp_url = link.get_attribute('href') if link else None
                    
                    if volume_match and ftp_url:
                        period_data = {
                            'volume': volume_match.group(1),
                            'issue': volume_match.group(2),
                            'date': date_match.group(1) if date_match else None,
                            'size': size_match.group(1) if size_match else None,
                            'ftp_url': ftp_url
                        }
                        year_data.append(period_data)
                        print(f"  âœ“ ç¬¬{period_data['volume']}å·{period_data['issue']}æœŸ")
                        
                except Exception as e:
                    continue
            
            all_data[year] = year_data
        
        # å„²å­˜çµæœ
        with open('tipo_links.json', 'w', encoding='utf-8') as f:
            json.dump(all_data, f, ensure_ascii=False, indent=2)
        
        browser.close()
        
        print(f"\nâœ“ å…±æ“·å– {sum(len(v) for v in all_data.values())} å€‹é€£çµ")
        return all_data

if __name__ == '__main__':
    data = scrape_tipo_detailed()
```

#### è¼¸å‡ºæ ¼å¼ç¯„ä¾‹

```json
{
  "114": [
    {
      "volume": "52",
      "issue": "20",
      "date": "114-10-18",
      "size": "77.22 MB",
      "ftp_url": "ftp://opdata.tipo.gov.tw/TrademarkReg/114/20/"
    },
    {
      "volume": "52",
      "issue": "19",
      "date": "114-10-02",
      "size": "107.85 MB",
      "ftp_url": "ftp://opdata.tipo.gov.tw/TrademarkReg/114/19/"
    }
  ],
  "113": [...]
}
```

### ä½¿ç”¨çˆ¬å–çµæœ

å–å¾—æ‰€æœ‰é€£çµå¾Œï¼Œå°±å¯ä»¥ï¼š
1. ç”¨ FileZilla æ‰‹å‹•ä¸‹è¼‰ç‰¹å®šæœŸåˆ¥
2. æ’°å¯«æ‰¹æ¬¡ä¸‹è¼‰è…³æœ¬ï¼ˆè¦‹æ–¹æ³•ä¸‰ï¼‰
3. åŒ¯å‡ºæˆ CSV ä¾›å…¶ä»–ç³»çµ±ä½¿ç”¨

---

## æ–¹æ³•ä¸‰ï¼šä½¿ç”¨ç¨‹å¼åŒ–æ‰¹æ¬¡ä¸‹è¼‰

### é‡è¦æé†’ï¼šå¤šå¹´ä»½ã€å¤šæœŸè³‡æ–™çµæ§‹

æ™ºæ…§è²¡ç”¢å±€çš„é–‹æ”¾è³‡æ–™ç¶²ç«™åŒ…å«ï¼š
- **å¤šå€‹å¹´ä»½**ï¼šå¾æ°‘åœ‹ 50 å¹´è‡³ä»Š
- **æ¯å¹´å¤šæœŸ**ï¼šæ¯æœŸå°æ‡‰ä¸€å€‹å…¬å ±ç™¼å¸ƒ
- **æ¯æœŸå¤šå€‹æª”æ¡ˆ**ï¼šXML æª”æ¡ˆã€åœ–æª”ç­‰

**FTP ç›®éŒ„çµæ§‹ç¯„ä¾‹**ï¼š
```
ftp://opdata.tipo.gov.tw/
â”œâ”€â”€ TrademarkReg/          # å•†æ¨™è¨»å†Šæ¡ˆå…¬å ±
â”‚   â”œâ”€â”€ 50/                # æ°‘åœ‹ 50 å¹´
â”‚   â”‚   â”œâ”€â”€ 1/            # ç¬¬ 1 æœŸ
â”‚   â”‚   â”œâ”€â”€ 2/            # ç¬¬ 2 æœŸ
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ 51/                # æ°‘åœ‹ 51 å¹´
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ 114/               # æ°‘åœ‹ 114 å¹´ï¼ˆ2025å¹´ï¼‰
â”‚       â””â”€â”€ ...
â”œâ”€â”€ PatentPub/             # ç™¼æ˜å°ˆåˆ©å…¬é–‹å…¬å ±
â””â”€â”€ PatentAnn/             # ç™¼æ˜å°ˆåˆ©å…¬å‘Šå…¬å ±
```

### ä½¿ç”¨ Python æ‰¹æ¬¡ä¸‹è¼‰

æœ‰äº†æ–¹æ³•äºŒçˆ¬å–çš„é€£çµæ¸…å–®å¾Œï¼Œå°±å¯ä»¥æ‰¹æ¬¡ä¸‹è¼‰æ‰€æœ‰æª”æ¡ˆã€‚

#### æ–¹æ¡ˆ 1ï¼šå¾ JSON è®€å–ä¸¦æ‰¹æ¬¡ä¸‹è¼‰

ä½¿ç”¨æ–¹æ³•äºŒç”¢ç”Ÿçš„ `tipo_links.json` é€²è¡Œæ‰¹æ¬¡ä¸‹è¼‰ã€‚

```python
from ftplib import FTP
import os
import json
from urllib.parse import urlparse

def download_from_json(json_file='tipo_links.json', output_dir='./downloads'):
    """
    å¾ JSON æª”æ¡ˆè®€å– FTP é€£çµä¸¦æ‰¹æ¬¡ä¸‹è¼‰
    
    Args:
        json_file: æ–¹æ³•äºŒç”¢ç”Ÿçš„ JSON æª”æ¡ˆ
        output_dir: æœ¬åœ°ä¸‹è¼‰ç›®éŒ„
    """
    # è®€å–é€£çµæ¸…å–®
    with open(json_file, 'r', encoding='utf-8') as f:
        all_links = json.load(f)
    
    print(f"è¼‰å…¥ {sum(len(v) for v in all_links.values())} å€‹ä¸‹è¼‰é€£çµ")
    
    # çµ±è¨ˆ
    total_files = 0
    success_files = 0
    
    for year, periods in all_links.items():
        print(f"\n========== è™•ç† {year} å¹´ ==========")
        
        for period in periods:
            ftp_url = period.get('ftp_url')
            if not ftp_url:
                continue
            
            # è§£æ FTP URL
            parsed = urlparse(ftp_url)
            host = parsed.netloc
            path = parsed.path
            
            volume = period.get('volume', 'unknown')
            issue = period.get('issue', 'unknown')
            
            print(f"\nä¸‹è¼‰ç¬¬ {volume} å· {issue} æœŸ...")
            
            # å»ºç«‹æœ¬åœ°ç›®éŒ„
            local_dir = os.path.join(output_dir, year, f"vol{volume}_iss{issue}")
            os.makedirs(local_dir, exist_ok=True)
            
            try:
                # é€£ç·š FTP
                ftp = FTP(host, timeout=300)
                ftp.login()
                ftp.cwd(path)
                
                # åˆ—å‡ºæª”æ¡ˆ
                files = ftp.nlst()
                print(f"  æ‰¾åˆ° {len(files)} å€‹æª”æ¡ˆ")
                
                # ä¸‹è¼‰æ¯å€‹æª”æ¡ˆ
                for filename in files:
                    local_path = os.path.join(local_dir, filename)
                    
                    try:
                        with open(local_path, 'wb') as f:
                            ftp.retrbinary(f'RETR {filename}', f.write)
                        print(f"    âœ“ {filename}")
                        success_files += 1
                    except Exception as e:
                        print(f"    âœ— {filename}: {e}")
                    
                    total_files += 1
                
                ftp.quit()
                
            except Exception as e:
                print(f"  âœ— é€£ç·šå¤±æ•—: {e}")
    
    print(f"\n========== å®Œæˆ ==========")
    print(f"æˆåŠŸä¸‹è¼‰: {success_files}/{total_files} å€‹æª”æ¡ˆ")

# ä½¿ç”¨ç¯„ä¾‹
if __name__ == '__main__':
    download_from_json('tipo_links.json', './tipo_data')
```

#### æ–¹æ¡ˆ 2ï¼šä¸‹è¼‰ç‰¹å®šå¹´ä»½æˆ–æœŸåˆ¥

```python
def download_specific_periods(json_file, year=None, issues=None, output_dir='./downloads'):
    """
    ä¸‹è¼‰ç‰¹å®šå¹´ä»½æˆ–æœŸåˆ¥
    
    Args:
        json_file: JSON æª”æ¡ˆè·¯å¾‘
        year: æŒ‡å®šå¹´ä»½ï¼ˆä¾‹ï¼š'114'ï¼‰ï¼ŒNone è¡¨ç¤ºå…¨éƒ¨
        issues: æŒ‡å®šæœŸåˆ¥åˆ—è¡¨ï¼ˆä¾‹ï¼š['20', '19']ï¼‰ï¼ŒNone è¡¨ç¤ºå…¨éƒ¨
        output_dir: è¼¸å‡ºç›®éŒ„
    """
    with open(json_file, 'r', encoding='utf-8') as f:
        all_links = json.load(f)
    
    # ç¯©é¸å¹´ä»½
    if year:
        all_links = {year: all_links.get(year, [])}
    
    for y, periods in all_links.items():
        # ç¯©é¸æœŸåˆ¥
        if issues:
            periods = [p for p in periods if p.get('issue') in issues]
        
        for period in periods:
            ftp_url = period['ftp_url']
            print(f"ä¸‹è¼‰: {y} å¹´ç¬¬ {period['issue']} æœŸ")
            # ... ä¸‹è¼‰é‚è¼¯åŒä¸Š ...

# ä½¿ç”¨ç¯„ä¾‹
# åªä¸‹è¼‰ 114 å¹´çš„è³‡æ–™
download_specific_periods('tipo_links.json', year='114')

# åªä¸‹è¼‰ç‰¹å®šæœŸåˆ¥
download_specific_periods('tipo_links.json', year='114', issues=['20', '19', '18'])
```

#### æ–¹æ¡ˆ 3ï¼šæ‰‹å‹•æŒ‡å®š FTP é€£çµä¸‹è¼‰

å¦‚æœæ²’æœ‰ä½¿ç”¨æ–¹æ³•äºŒçˆ¬å–ï¼Œä¹Ÿå¯ä»¥æ‰‹å‹•æŒ‡å®šé€£çµã€‚

```python
from ftplib import FTP
import os

def download_single_period(ftp_url, local_dir):
    """ä¸‹è¼‰å–®ä¸€æœŸåˆ¥çš„æ‰€æœ‰æª”æ¡ˆ
    
    Args:
        ftp_url: FTP é€£çµï¼Œä¾‹å¦‚ 'ftp://opdata.tipo.gov.tw/TrademarkReg/114/20/'
        local_dir: æœ¬åœ°ä¸‹è¼‰ç›®éŒ„
    """
    from urllib.parse import urlparse
    
    # è§£æ URL
    parsed = urlparse(ftp_url)
    host = parsed.netloc
    path = parsed.path
    
    # å»ºç«‹æœ¬åœ°ç›®éŒ„
    os.makedirs(local_dir, exist_ok=True)
    
    # é€£ç·šåˆ° FTP ä¼ºæœå™¨
    ftp = FTP(host, timeout=300)
    ftp.login()  # åŒ¿åç™»å…¥
    ftp.cwd(path)
    
    # åˆ—å‡ºæª”æ¡ˆ
    files = ftp.nlst()
    print(f"æ‰¾åˆ° {len(files)} å€‹æª”æ¡ˆ")
    
    # ä¸‹è¼‰æ‰€æœ‰æª”æ¡ˆ
    for filename in files:
        local_path = os.path.join(local_dir, filename)
        print(f"æ­£åœ¨ä¸‹è¼‰: {filename}")
        
        try:
            with open(local_path, 'wb') as f:
                ftp.retrbinary(f'RETR {filename}', f.write)
            print(f"âœ“ å®Œæˆ: {filename}")
        except Exception as e:
            print(f"âœ— å¤±æ•—: {filename} - {e}")
    
    # é—œé–‰é€£ç·š
    ftp.quit()
    print("ä¸‹è¼‰å®Œæˆï¼")

# ä½¿ç”¨ç¯„ä¾‹
download_single_period(
    'ftp://opdata.tipo.gov.tw/TrademarkReg/114/20/',
    './downloads/114/20'
)
```

#### æ–¹æ¡ˆ 4ï¼šé€²éšæ‰¹æ¬¡ä¸‹è¼‰å™¨ï¼ˆå«é€²åº¦æ¢ã€æ–·é»çºŒå‚³ï¼‰

```python
from ftplib import FTP
import os
import json
from tqdm import tqdm  # éœ€è¦: pip install tqdm
from urllib.parse import urlparse

class AdvancedTIPODownloader:
    """é€²éšä¸‹è¼‰å™¨ï¼šæ”¯æ´é€²åº¦æ¢ã€æ–·é»çºŒå‚³ã€éŒ¯èª¤é‡è©¦"""
    
    def __init__(self, json_file='tipo_links.json'):
        self.json_file = json_file
        self.download_log = 'download_log.json'
        self.load_progress()
    
    def load_progress(self):
        """è¼‰å…¥ä¸‹è¼‰é€²åº¦"""
        if os.path.exists(self.download_log):
            with open(self.download_log, 'r') as f:
                self.completed = json.load(f)
        else:
            self.completed = {}
    
    def save_progress(self):
        """å„²å­˜ä¸‹è¼‰é€²åº¦"""
        with open(self.download_log, 'w') as f:
            json.dump(self.completed, f, indent=2)
    
    def download_with_progress(self, output_dir='./downloads', max_retries=3):
        """å¸¶é€²åº¦æ¢çš„æ‰¹æ¬¡ä¸‹è¼‰"""
        
        # è®€å–é€£çµæ¸…å–®
        with open(self.json_file, 'r', encoding='utf-8') as f:
            all_links = json.load(f)
        
        # è¨ˆç®—ç¸½ä»»å‹™æ•¸
        total_periods = sum(len(periods) for periods in all_links.values())
        
        with tqdm(total=total_periods, desc="ç¸½é€²åº¦") as pbar:
            for year, periods in all_links.items():
                for period in periods:
                    period_key = f"{year}_{period.get('volume')}_{period.get('issue')}"
                    
                    # æª¢æŸ¥æ˜¯å¦å·²ä¸‹è¼‰
                    if period_key in self.completed:
                        pbar.update(1)
                        continue
                    
                    ftp_url = period['ftp_url']
                    local_dir = os.path.join(
                        output_dir, year, 
                        f"vol{period['volume']}_iss{period['issue']}"
                    )
                    
                    # é‡è©¦æ©Ÿåˆ¶
                    for attempt in range(max_retries):
                        try:
                            self.download_period(ftp_url, local_dir)
                            self.completed[period_key] = True
                            self.save_progress()
                            break
                        except Exception as e:
                            if attempt == max_retries - 1:
                                print(f"âœ— å¤±æ•— {period_key}: {e}")
                            else:
                                print(f"é‡è©¦ {attempt + 1}/{max_retries}...")
                    
                    pbar.update(1)
    
    def download_period(self, ftp_url, local_dir):
        """ä¸‹è¼‰å–®ä¸€æœŸåˆ¥"""
        from urllib.parse import urlparse
        
        parsed = urlparse(ftp_url)
        ftp = FTP(parsed.netloc, timeout=300)
        ftp.login()
        ftp.cwd(parsed.path)
        
        files = ftp.nlst()
        os.makedirs(local_dir, exist_ok=True)
        
        for filename in files:
            local_path = os.path.join(local_dir, filename)
            
            # æ–·é»çºŒå‚³ï¼šæª¢æŸ¥æª”æ¡ˆæ˜¯å¦å·²å­˜åœ¨
            if os.path.exists(local_path):
                continue
            
            with open(local_path, 'wb') as f:
                ftp.retrbinary(f'RETR {filename}', f.write)
        
        ftp.quit()

# ä½¿ç”¨ç¯„ä¾‹
downloader = AdvancedTIPODownloader('tipo_links.json')
downloader.download_with_progress('./tipo_data')
```

---

### ä½¿ç”¨ curl æˆ– wget ä¸‹è¼‰

#### curl æ‰¹æ¬¡ä¸‹è¼‰

```bash
# å¾ JSON è®€å–ä¸¦ä¸‹è¼‰ï¼ˆéœ€æ­é… jq å·¥å…·ï¼‰
cat tipo_links.json | jq -r '.[][] | .ftp_url' | while read url; do
    echo "ä¸‹è¼‰: $url"
    curl -O "$url"
done
```

#### wget æ‰¹æ¬¡ä¸‹è¼‰

```bash
# ä¸‹è¼‰æ•´å€‹ FTP ç›®éŒ„
wget -r -np -nH --cut-dirs=3 ftp://opdata.tipo.gov.tw/TrademarkReg/114/20/

# åƒæ•¸èªªæ˜ï¼š
# -r: éè¿´ä¸‹è¼‰
# -np: ä¸è¿½æº¯åˆ°çˆ¶ç›®éŒ„
# -nH: ä¸å»ºç«‹ä¸»æ©Ÿåç¨±ç›®éŒ„
# --cut-dirs=3: è·³éå‰ä¸‰å±¤ç›®éŒ„çµæ§‹
```

---

## å®Œæ•´å·¥ä½œæµç¨‹ç¯„ä¾‹

### æ­¥é©Ÿ 1ï¼šçˆ¬å–æ‰€æœ‰ä¸‹è¼‰é€£çµ

```bash
# ä½¿ç”¨ Playwright çˆ¬å–
python scrape_tipo.py
# ç”¢ç”Ÿ tipo_links.json
```

### æ­¥é©Ÿ 2ï¼šæª¢è¦–é€£çµæ¸…å–®

```bash
# æŸ¥çœ‹ JSON å…§å®¹
cat tipo_links.json | jq '.'

# çµ±è¨ˆå„å¹´ä»½æœŸæ•¸
cat tipo_links.json | jq 'to_entries | map({year: .key, count: (.value | length)})'
```

### æ­¥é©Ÿ 3ï¼šæ‰¹æ¬¡ä¸‹è¼‰

```bash
# ä½¿ç”¨ Python ä¸‹è¼‰å™¨
python batch_download.py

# æˆ–ä½¿ç”¨é€²éšä¸‹è¼‰å™¨ï¼ˆå«é€²åº¦æ¢ï¼‰
python advanced_download.py
```

### æ­¥é©Ÿ 4ï¼šé©—è­‰ä¸‹è¼‰å®Œæ•´æ€§

```python
import os
import json

def verify_downloads(json_file, download_dir):
    """é©—è­‰ä¸‹è¼‰å®Œæ•´æ€§"""
    with open(json_file, 'r') as f:
        all_links = json.load(f)
    
    missing = []
    
    for year, periods in all_links.items():
        for period in periods:
            vol = period['volume']
            iss = period['issue']
            local_dir = os.path.join(download_dir, year, f"vol{vol}_iss{iss}")
            
            if not os.path.exists(local_dir):
                missing.append(f"{year}/vol{vol}_iss{iss}")
    
    if missing:
        print(f"ç¼ºå°‘ {len(missing)} å€‹æœŸåˆ¥:")
        for m in missing:
            print(f"  - {m}")
    else:
        print("âœ“ æ‰€æœ‰æª”æ¡ˆä¸‹è¼‰å®Œæˆï¼")

verify_downloads('tipo_links.json', './tipo_data')
```

---

## è³‡æ–™é›†èªªæ˜

### ä½¿ç”¨ curl ä¸‹è¼‰

é©åˆ Linux/Mac ä½¿ç”¨è€…æˆ–éœ€è¦ç°¡å–®è…³æœ¬çš„æƒ…æ³ã€‚

```bash
# ä¸‹è¼‰å–®ä¸€æª”æ¡ˆ
curl -O ftp://opdata.tipo.gov.tw/TrademarkReg/50/5/æª”æ¡ˆåç¨±.xml

# æ‰¹æ¬¡ä¸‹è¼‰æ•´å€‹ç›®éŒ„ï¼ˆéœ€è¦å…ˆåˆ—å‡ºæª”æ¡ˆæ¸…å–®ï¼‰
wget -r ftp://opdata.tipo.gov.tw/TrademarkReg/50/5/
```

### ä½¿ç”¨ wget æ‰¹æ¬¡ä¸‹è¼‰

```bash
# éè¿´ä¸‹è¼‰æ•´å€‹ç›®éŒ„
wget -r -np -nH --cut-dirs=2 ftp://opdata.tipo.gov.tw/TrademarkReg/50/5/

# åƒæ•¸èªªæ˜ï¼š
# -r: éè¿´ä¸‹è¼‰
# -np: ä¸è¿½æº¯åˆ°çˆ¶ç›®éŒ„
# -nH: ä¸å»ºç«‹ä¸»æ©Ÿåç¨±ç›®éŒ„
# --cut-dirs=2: è·³éå‰å…©å±¤ç›®éŒ„çµæ§‹
```

---

## è³‡æ–™é›†èªªæ˜

### å•†æ¨™è³‡æ–™é›†

#### 1. å•†æ¨™è¨»å†Šæ¡ˆå…¬å ±ï¼ˆ1æ¡ˆ1XMLæª”ï¼‰
- **è³‡æ–™å…§å®¹**ï¼šå•†æ¨™ç¨®é¡ã€è¨»å†Šè™Ÿã€ç”³è«‹æ¡ˆè™Ÿã€å…¬å‘Šå·æœŸã€ç”³è«‹æ—¥æœŸã€å•†æ¨™åç¨±ã€å•†æ¨™æ¬Šäººè³‡è¨Šã€ä»£ç†äººè³‡è¨Šã€æ¬Šåˆ©æœŸé–“ã€å•†æ¨™åœ–æ¨£ã€å•†å“æˆ–æœå‹™é¡åˆ¥ç­‰
- **æª”æ¡ˆæ ¼å¼**ï¼šXMLã€WAVã€JPEG
- **é©ç”¨æƒ…å¢ƒ**ï¼šéœ€è¦æŸ¥è©¢ç‰¹å®šå•†æ¨™æ¡ˆä»¶
- **æ›´æ–°é »ç‡**ï¼šå®šæœŸæ›´æ–°ï¼ˆé€šå¸¸æ¯æœŸå…¬å ±ç™¼å¸ƒå¾Œï¼‰

#### 2. å•†æ¨™è¨»å†Šæ¡ˆå…¬å ±ï¼ˆ1å·æœŸ1XMLæª”ï¼‰
- **è³‡æ–™å…§å®¹**ï¼šèˆ‡ã€Œ1æ¡ˆ1XMLæª”ã€ç›¸åŒï¼Œä½†æ•´æœŸå…¬å ±æ‰“åŒ…åœ¨ä¸€å€‹æª”æ¡ˆ
- **æª”æ¡ˆæ ¼å¼**ï¼šXMLã€WAVã€JPEG
- **é©ç”¨æƒ…å¢ƒ**ï¼šéœ€è¦æ‰¹æ¬¡ä¸‹è¼‰æŸæœŸå…¬å ±çš„æ‰€æœ‰æ¡ˆä»¶
- **æ›´æ–°é »ç‡**ï¼šå®šæœŸæ›´æ–°

#### 3. å•†æ¨™è¨»å†Šæ¡ˆå…¬å ±ï¼ˆå–®é å¼æƒææª”ï¼‰
- **è³‡æ–™å…§å®¹**ï¼šå•†æ¨™è¨»å†Šæ¡ˆå…¬å ±æƒæåœ–æª”è³‡æ–™
- **æª”æ¡ˆæ ¼å¼**ï¼šXMLã€TIFF
- **é©ç”¨æƒ…å¢ƒ**ï¼šéœ€è¦æŸ¥çœ‹åŸå§‹å…¬å ±å½±åƒ

### å°ˆåˆ©è³‡æ–™é›†

#### ç™¼æ˜å°ˆåˆ©å…¬é–‹å…¬å ±
- åŒ…å«å°ˆåˆ©ç”³è«‹æ¡ˆåœ¨å…¬é–‹éšæ®µçš„è³‡è¨Š
- é€šå¸¸åœ¨ç”³è«‹å¾Œ 18 å€‹æœˆå…¬é–‹

#### ç™¼æ˜å°ˆåˆ©å…¬å‘Šå…¬å ±
- åŒ…å«å·²ç²å‡†å°ˆåˆ©æ¬Šçš„è³‡è¨Š
- æä¾›å®Œæ•´çš„å°ˆåˆ©èªªæ˜æ›¸å…§å®¹

### è³‡æ–™æ ¼å¼èªªæ˜

- **XML**ï¼šçµæ§‹åŒ–è³‡æ–™ï¼Œé©åˆç¨‹å¼è§£æ
- **TIFF/JPEG**ï¼šåœ–æª”æ ¼å¼ï¼Œç”¨æ–¼å•†æ¨™åœ–æ¨£æˆ–æƒææª”
- **WAV**ï¼šéŸ³æ•ˆæª”ï¼ˆéƒ¨åˆ†å•†æ¨™åŒ…å«è²éŸ³å•†æ¨™ï¼‰

---

## å¸¸è¦‹å•é¡Œ

### Q1ï¼šç‚ºä»€éº¼ç€è¦½å™¨ç„¡æ³•é–‹å•Ÿ FTP é€£çµï¼Ÿ
**A**ï¼šç¾ä»£ç€è¦½å™¨å·²ä¸æ”¯æ´ FTP å”å®šï¼Œå»ºè­°ä½¿ç”¨ FileZilla ç­‰ FTP è»Ÿé«”ï¼Œæˆ–ä½¿ç”¨ç¨‹å¼åŒ–æ–¹å¼ä¸‹è¼‰ã€‚

### Q2ï¼šä¸‹è¼‰é€Ÿåº¦å¾ˆæ…¢æˆ–é€£ç·šé€¾æ™‚æ€éº¼è¾¦ï¼Ÿ
**A**ï¼š
1. èª¿æ•´ FileZilla é€¾æ™‚è¨­å®šç‚º 240 ç§’æˆ– 0 ç§’
2. é¿å…åœ¨å°–å³°æ™‚æ®µä¸‹è¼‰
3. ä½¿ç”¨ç¨‹å¼åŒ–ä¸‹è¼‰ä¸¦åŠ å…¥é‡è©¦æ©Ÿåˆ¶

### Q3ï¼šå¦‚ä½•æ‰¾åˆ°ç‰¹å®šå°ˆåˆ©æˆ–å•†æ¨™çš„è³‡æ–™ï¼Ÿ
**A**ï¼š
1. å…ˆåœ¨æ™ºæ…§è²¡ç”¢å±€çš„æª¢ç´¢ç³»çµ±æŸ¥è©¢ï¼ˆå–å¾—ç”³è«‹è™Ÿæˆ–å…¬å‘Šè™Ÿï¼‰
2. æ ¹æ“šå…¬å‘Šæ—¥æœŸæ‰¾åˆ°å°æ‡‰çš„å·æœŸ
3. ä¸‹è¼‰è©²å·æœŸçš„è³‡æ–™å¾Œå†é€²è¡Œç¯©é¸

### Q4ï¼šè³‡æ–™æ›´æ–°é »ç‡å¦‚ä½•ï¼Ÿ
**A**ï¼šé€šå¸¸é…åˆå…¬å ±ç™¼å¸ƒæ™‚é–“æ›´æ–°ï¼š
- å•†æ¨™å…¬å ±ï¼šæ¯æœŸç™¼å¸ƒå¾Œ
- å°ˆåˆ©å…¬å ±ï¼šæ¯é€±æˆ–æ¯æœˆï¼ˆä¾é¡å‹è€Œå®šï¼‰

### Q5ï¼šä¸‹è¼‰çš„ XML æª”æ¡ˆå¦‚ä½•è§£æï¼Ÿ
**A**ï¼šå¯ä½¿ç”¨ä»¥ä¸‹æ–¹å¼ï¼š
- Python: `xml.etree.ElementTree` æˆ– `lxml`
- JavaScript: `DOMParser` æˆ– `xml2js`
- Java: `DocumentBuilder`
- ä»»ä½•æ”¯æ´ XML è§£æçš„ç¨‹å¼èªè¨€

### Q6ï¼šè³‡æ–™å¯ä»¥å•†ç”¨å—ï¼Ÿ
**A**ï¼šæ˜¯çš„ï¼Œå±¬æ–¼æ”¿åºœé–‹æ”¾è³‡æ–™ï¼Œéµå¾ªã€Œæ”¿åºœè³‡æ–™é–‹æ”¾æˆæ¬Šæ¢æ¬¾ã€å³å¯è‡ªç”±ä½¿ç”¨ã€ä¿®æ”¹ã€æ•£å¸ƒã€‚ä½†è«‹æ³¨æ„ï¼š
- è³‡æ–™åƒ…ä¾›åƒè€ƒï¼Œä¸ä½œç‚ºæ³•å¾‹ä¾æ“š
- å»ºè­°æ¨™è¨»è³‡æ–™ä¾†æº

### Q7ï¼šå¦‚ä½•å–å¾—æœ€æ–°çš„è³‡æ–™é›†æ¸…å–®ï¼Ÿ
**A**ï¼šå®šæœŸæª¢æŸ¥é–‹æ”¾è³‡æ–™ç¶²ç«™ï¼Œæˆ–ä½¿ç”¨ç¨‹å¼è‡ªå‹•çˆ¬å–ç›®éŒ„æ¸…å–®ã€‚

---

## è¯çµ¡è³‡è¨Š

å¦‚æœ‰ä»»ä½•å•é¡Œï¼Œè«‹è¯ç¹«æ™ºæ…§è²¡ç”¢å±€ï¼š

- **åœ°å€**ï¼š106213 è‡ºåŒ—å¸‚å¤§å®‰å€è¾›äº¥è·¯2æ®µ185è™Ÿ3æ¨“
- **é›»è©±**ï¼š(02) 2738-0007
- **å‚³çœŸ**ï¼š(02) 2377-9875
- **å°ˆåˆ©å•†æ¨™è³‡æ–™åº«æª¢ç´¢æœå‹™å°**ï¼š(02) 2376-7165ã€(02) 2376-7166
- **æœå‹™æ™‚é–“**ï¼šé€±ä¸€è‡³é€±äº” 08:30-12:30ã€13:30-17:30

---

## ç›¸é—œè³‡æº

- [æ™ºæ…§è²¡ç”¢å±€å®˜ç¶²](https://www.tipo.gov.tw)
- [å…¨çƒå°ˆåˆ©æª¢ç´¢ç³»çµ± GPSS](https://gpss.tipo.gov.tw)
- [å°ˆåˆ©å•†æ¨™é–‹æ”¾è³‡æ–™](https://cloud.tipo.gov.tw/S220/opdata)
- [æ”¿åºœè³‡æ–™é–‹æ”¾å¹³å°](https://data.gov.tw)
- [æ™ºæ…§è²¡ç”¢å±€ Facebook](https://www.facebook.com/TIPO.gov.tw/)

---

## ç‰ˆæœ¬è³‡è¨Š

- **æ–‡ä»¶ç‰ˆæœ¬**ï¼š1.0
- **æœ€å¾Œæ›´æ–°**ï¼š2025å¹´10æœˆ
- **æ’°å¯«è€…**ï¼šä¾æ“šæ™ºæ…§è²¡ç”¢å±€é–‹æ”¾è³‡æ–™ç¶²ç«™è³‡è¨Šæ•´ç†

---

**æˆæ¬Šè²æ˜**ï¼šæœ¬æ•™å­¸æ–‡ä»¶ä¾æ“šæ”¿åºœè³‡æ–™é–‹æ”¾æˆæ¬Šæ¢æ¬¾æä¾›ï¼Œæ­¡è¿è‡ªç”±ä½¿ç”¨ã€ä¿®æ”¹èˆ‡æ•£å¸ƒã€‚